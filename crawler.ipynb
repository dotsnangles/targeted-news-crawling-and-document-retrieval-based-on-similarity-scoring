{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import time\n",
    "import re\n",
    "import argparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'hwEnzlSdGXZTyeubesmy'\n",
    "client_secret = 'WkpCvqbw6u'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Naver API에서 뉴스 항목을 불러옵니다.\n",
    "\n",
    "def get_news_list(crawling_trg, client_id, client_secret):\n",
    "    news_list = []\n",
    "    for step in range(1, 1100, 100):\n",
    "        if step == 1001:\n",
    "            step = 1000\n",
    "        \n",
    "        params = dict(\n",
    "            query=urllib.parse.quote(crawling_trg),\n",
    "            display=urllib.parse.quote('100'), \n",
    "            start=urllib.parse.quote(str(step)),\n",
    "            sort=urllib.parse.quote('date'),\n",
    "        )\n",
    "\n",
    "        url = f\"https://openapi.naver.com/v1/search/news?query={params['query']}&display={params['display']}&start={params['start']}&sort={params['sort']}\"\n",
    "\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header('X-Naver-Client-Id',client_id)\n",
    "        request.add_header('X-Naver-Client-Secret',client_secret)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "\n",
    "        if(rescode==200):\n",
    "            response_body = response.read()\n",
    "            result = json.loads(response_body.decode('utf-8'))['items']\n",
    "            news_list.extend(result)\n",
    "        else:\n",
    "            print('Error Code:' + rescode)\n",
    "        # print(step)\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        if step == 101:\n",
    "            break\n",
    "    \n",
    "    news_list_df = pd.DataFrame(news_list).drop_duplicates()\n",
    "    news_list_df['checker'] = news_list_df.link.str.find('https://n.news.naver.com/')\n",
    "    news_list_df = news_list_df[news_list_df['checker'] > -1]\n",
    "    news_list_df.reset_index(inplace=True)\n",
    "    news_list_df['crawling_trg'] = crawling_trg\n",
    "    \n",
    "    return news_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "    'cookie': 'nid_inf=1417715440; NID_AUT=jB6JKkl0bQJkzabA2sqcsZPcc58lN3Nk/oKjGyVYl1n8PedB7ujlxHdECKcm9kS7; NID_JKL=CdIfrElqYRkes7uT/S10prytLgVd7C1HBLS0nz55Sys=; VISIT_LOG_CLEAN=1; NFS=2; NID_SES=AAABkxQsSdlab5VkBVUMJHd12mikXaT/HRo/rtzzaxgjXmXJLfOFG25S5TiaZsFNk5bLeRF4Ej3UoZTevQC7Jd7fVFOAFJtSFoNgapi8qqUTVHDqzpWo4zUjmx/LD5cdsWbvaL/cByCai8XYLRLdB0o6W7sQutPmRMFUblZMKU/CdpBh8aziLA/i9wHRgkCr4zpYxc/YROLyZLZ7QmphI5qakKYHN4OxNKYy/Dp+PvlCsWk6azoXw7AxABwR/NdLMwDxNgZ6yN0iUJDkWFFPEN0yBGCm5VaOa1oF4tU0GkLMQsbyMb/+mOw6aBjxQNSgS5RDsn3Vs6H9l1qczNtasU4OpdonIK0EzoLYyQEgLkpXeLTnzkdaJnqR7EyjKjl8hH/EsaVdnRaAEs0afY3VK8f+pCn0z/5n9xpiCd0UVrpoI3nwF1No4x/mEk71EW2ghH90hyg/9F0KHAHLneYZ6lCvcdhE/ENHrmc1nP0R0sUnOltV0XIyf0fEua7rvSqv0P0iuE0xD2+WUqh//96WFUP+XyeQxvn16XPBT8PSfE/nU/FQ; nx_ssl=2; page_uid=h1WRNsp0YiRssKvZbrNssssssMh-389234; BMR='\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Naver API에서 불러온 뉴스 항목의 본문을 불러옵니다.\n",
    "\n",
    "def crawl_news(news_list_df, headers):\n",
    "    crawled_news = []\n",
    "    for idx, row in news_list_df.iterrows():\n",
    "        # row.crawling_trg, row.pubDate, row.title, cleansed_news_body, row.originallink, row.link, row.description\n",
    "        URL = str(row.link)\n",
    "        r = requests.get(URL, headers=headers)\n",
    "        \n",
    "        try:\n",
    "            soup = BeautifulSoup(r.content, 'html5lib')\n",
    "            news_body = soup.select('#newsct_article')[0].text\n",
    "            cleansed_news_body = re.sub(r'[\\n\\t^/$]', '', news_body)\n",
    "            crawled_news.append([row.crawling_trg, row.pubDate, row.title, cleansed_news_body, row.originallink, row.link, row.description])\n",
    "        except Exception as e:\n",
    "            soup = BeautifulSoup(r.content, 'html5lib')\n",
    "            news_body = soup.select('#articeBody')[0].text\n",
    "            cleansed_news_body = re.sub(r'[\\n\\t^/$]', '', news_body)\n",
    "            crawled_news.append([row.crawling_trg, row.pubDate, row.title, cleansed_news_body, row.originallink, row.link, row.description])\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        if idx == 3:\n",
    "            break\n",
    "    \n",
    "    crawled_news_df = pd.DataFrame(crawled_news, columns=['crawling_trg', 'pubDate', 'title', 'body', 'originallink', 'link', 'description'])\n",
    "    return crawled_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('government_organization_chart.json', encoding='utf-8') as f:\n",
    "    gov_orgs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--query\", dest=\"query\", action=\"store\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "query = '인공지능'\n",
    "org_name = '기획재정부'\n",
    "sub_orgs = gov_orgs[org_name]\n",
    "\n",
    "crawling_trgs = []\n",
    "for sub_org in sub_orgs:\n",
    "    crawling_trg = ' '.join([org_name, sub_org, query])\n",
    "    crawling_trgs.append(crawling_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_news_dfs = []\n",
    "for crawling_trg in crawling_trgs:\n",
    "    news_list_df = get_news_list(crawling_trg, client_id, client_secret)\n",
    "    crawled_news_df = crawl_news(news_list_df, headers)\n",
    "    crawled_news_dfs.append(crawled_news_df)\n",
    "    \n",
    "result = pd.concat(crawled_news_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sub_org = result.body.str.contains('|'.join(sub_orgs), case=False, regex=True)\n",
    "search_query = result.body.str.contains(query, case=False, regex=True)\n",
    "result = result[search_sub_org & search_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(f'{\"_\".join([query, org_name])}_crawled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result =pd.read_csv('인공지능_기획재정부_crawled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('인공지능',\n",
       " ['한국수출입은행',\n",
       "  '한국조폐공사',\n",
       "  '한국재정정보원',\n",
       "  '한국투자공사',\n",
       "  '복권위원회사무처',\n",
       "  '국세청',\n",
       "  '관세청',\n",
       "  '조달청',\n",
       "  '통계청'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query, sub_orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_sub_org = result.body.str.contains('|'.join(sub_orgs), case=False, regex=True)\n",
    "search_query = result.body.str.contains(query, case=False, regex=True)\n",
    "len(result[search_sub_org & search_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_condition = (result.body.str.contains('|'.join(sub_orgs), case=False, regex=True)) & (result.body.str.contains(query, case=False, regex=True))\n",
    "len(result[search_condition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('jeonghyeon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
