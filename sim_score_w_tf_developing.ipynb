{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### 모델과 토크나이저를 SentenceTransformer 클래스로 불러옵니다.\n",
    "pretrained_model_path = \"sbert/training_klue_sts_klue-roberta-base-2022-08-17_23-27-13\"\n",
    "model = SentenceTransformer(pretrained_model_path)\n",
    "\n",
    "def id_by_tf_retrieve(df, keyword_th=0, name_th=0):\n",
    "    keyword = df.keyword.unique()[0]\n",
    "    name = df.sub_org.unique()[0]\n",
    "\n",
    "    keyword_tfs = df.content.str.count(keyword).tolist()\n",
    "    name_tfs = df.content.str.count(name).tolist()\n",
    "\n",
    "    tf_pairs = [(q, n) for q, n in zip(keyword_tfs, name_tfs)]\n",
    "    ids = {}\n",
    "    for id, tf_pair in enumerate(tf_pairs):\n",
    "        keyword_tf, name_tf = tf_pair[0], tf_pair[1]\n",
    "        if keyword_tf >= keyword_th and name_tf >= name_th:\n",
    "            ids[id] = keyword_tfs[id] + name_tfs[id]\n",
    "    if bool(ids) == False:\n",
    "        return None, None\n",
    "    \n",
    "    doc_id = max(ids, key=ids.get)\n",
    "    tf_score = max(ids.values())\n",
    "\n",
    "    return doc_id, tf_score\n",
    "\n",
    "### 유사도를 계산하여 인덱스와 스코어를 반환하는 함수입니다.\n",
    "def get_indices_and_scores(query, news_contents, top_k):\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.encode(query)\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "        news_embeddings = model.encode(news_contents.content)\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, news_embeddings).squeeze()\n",
    "\n",
    "        top_k = min(top_k, len(news_contents.content))\n",
    "        top_k_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        scores = top_k_results.values.squeeze()\n",
    "        indices = top_k_results.indices.squeeze()\n",
    "\n",
    "    return indices, scores\n",
    "\n",
    "### 유사도 점수에 기반하여 문서를 찾아옵니다.\n",
    "\n",
    "def retrieve_docs(keyword, business_name, crawling_result):\n",
    "    ### sub_org 칼럼에 business_name 이름이 있는 것과 없는 것을 구분하여 데이터를 나눕니다.\n",
    "    crawling_result['checker'] = crawling_result.sub_org.str.find(business_name)\n",
    "\n",
    "    bussiness_news = crawling_result[crawling_result.checker > -1].copy().reset_index(drop=True)\n",
    "    bussiness_news['idx_original'] = range(len(bussiness_news))\n",
    "\n",
    "    org_news = crawling_result[crawling_result.checker == -1].copy().reset_index(drop=True)\n",
    "    org_news['idx_original'] = range(len(org_news))\n",
    "\n",
    "    business_news_contents = bussiness_news[['idx_original', 'sub_org', 'keyword', 'pubDate', 'title', 'content', 'link']]\n",
    "    org_news_contents = org_news[['idx_original', 'sub_org', 'keyword', 'pubDate', 'title', 'content', 'link']]\n",
    "    \n",
    "    ### 비지니스명과 쿼리의 term frequency threshold를 충족한 기사 중 term frequency의 합계가 가장 높은 기사를 가지고 옵니다.(중복시 첫 번째)\n",
    "    doc_id, tf_score = id_by_tf_retrieve(business_news_contents, keyword_th=1, name_th=2)\n",
    "    if doc_id == None:\n",
    "        print(f'{business_name}: 주목할 만한 기사 없음.')\n",
    "        print('프로그램을 종료합니다.')\n",
    "        return None\n",
    "    else:\n",
    "        top_of_business_news_contents = business_news_contents.iloc[doc_id].to_list() + list([tf_score])\n",
    "        top_of_business_news_contents = pd.DataFrame([top_of_business_news_contents], columns=['idx_original', 'sub_org', 'keyword', 'pubDate', 'title', 'content', 'link', 'score'])\n",
    "\n",
    "    ### 기관별로 데이터를 나눕니다.\n",
    "    org_news_contents_splits = []\n",
    "    for org in org_news_contents.sub_org.unique():\n",
    "        org_news_contents_split = org_news_contents[org_news_contents.sub_org == org].reset_index(drop=True).copy()\n",
    "        org_news_contents_splits.append(org_news_contents_split)\n",
    "\n",
    "    ### 각 기관별 수행. 기관명과 쿼리의 term frequency threshold를 충족한 기사 중 term frequency의 합계가 가장 높은 기사를 가지고 옵니다.(중복시 첫 번째)\n",
    "    tops_of_org_news_contents_splits = []\n",
    "    for org_news_contents_split in org_news_contents_splits:\n",
    "        sub_org_name = org_news_contents_split.sub_org.unique()[0]\n",
    "        doc_id, tf_score = id_by_tf_retrieve(org_news_contents_split, keyword_th=2, name_th=3)\n",
    "        if doc_id == None:\n",
    "            print(f'{sub_org_name}: 주목할 만한 기사 없음.')\n",
    "            print()\n",
    "            continue\n",
    "        top_of_org_news_contents_split = org_news_contents_split.iloc[doc_id].to_list() + list([tf_score])\n",
    "        tops_of_org_news_contents_splits.append(top_of_org_news_contents_split)\n",
    "\n",
    "    if len(tops_of_org_news_contents_splits) == 0:\n",
    "        print('검색한 상위 기관에 대하여 주목할 만한 기사 없음.')\n",
    "        print('프로그램을 종료합니다.')\n",
    "        return None\n",
    "    \n",
    "    tops_of_org_news_contents_splits = pd.DataFrame(tops_of_org_news_contents_splits, columns=['idx_original', 'sub_org', 'keyword', 'pubDate', 'title', 'content', 'link', 'score'])\n",
    "    tops_of_org_news_contents_splits = tops_of_org_news_contents_splits.sort_values('score', ascending=False).reset_index(drop=True).copy()\n",
    "\n",
    "    ### 가장 높은 점수의 business_name 문서를 쿼리로 하여 \n",
    "    ### 가장 높은 점수의 기관별 문서 뭉치와 유사도 점수를 계산하고 유사도 점수 상위 5개 문서를 찾아옵니다.\n",
    "    indices, scores = get_indices_and_scores(top_of_business_news_contents.content.iloc[0], tops_of_org_news_contents_splits, 5)\n",
    "    result = tops_of_org_news_contents_splits.iloc[list(indices)].copy()\n",
    "    result['score'] = scores\n",
    "\n",
    "    return top_of_business_news_contents, tops_of_org_news_contents_splits, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['sub_org', 'query', 'pubDate', 'title', 'content', 'originallink', 'link', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        국립국어원\n",
       "1        국립국어원\n",
       "2        국립국어원\n",
       "3        국립국어원\n",
       "4      국립민속박물관\n",
       "        ...   \n",
       "99     한국문화재재단\n",
       "100        명필름\n",
       "101        명필름\n",
       "102        명필름\n",
       "103        명필름\n",
       "Name: sub_org, Length: 104, dtype: object"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "keyword = '영화'\n",
    "business_name = '명필름'\n",
    "crawling_result = pd.read_csv('영화_문화체육관광부_명필름_crawled.csv')\n",
    "crawling_result.sub_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국립국어원: 주목할 만한 기사 없음.\n",
      "\n",
      "국립민속박물관: 주목할 만한 기사 없음.\n",
      "\n",
      "국립아시아문화전당: 주목할 만한 기사 없음.\n",
      "\n",
      "국립장애인도서관: 주목할 만한 기사 없음.\n",
      "\n",
      "국립중앙박물관: 주목할 만한 기사 없음.\n",
      "\n",
      "국립한글박물관: 주목할 만한 기사 없음.\n",
      "\n",
      "한국예술종합학교: 주목할 만한 기사 없음.\n",
      "\n",
      "해외문화홍보원: 주목할 만한 기사 없음.\n",
      "\n",
      "대한체육회: 주목할 만한 기사 없음.\n",
      "\n",
      "사행산업통합감독위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "세종학당재단: 주목할 만한 기사 없음.\n",
      "\n",
      "영상물등급위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "영화진흥위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "재단법인국악방송: 주목할 만한 기사 없음.\n",
      "\n",
      "태권도진흥재단: 주목할 만한 기사 없음.\n",
      "\n",
      "한국관광공사: 주목할 만한 기사 없음.\n",
      "\n",
      "한국문학번역원: 주목할 만한 기사 없음.\n",
      "\n",
      "한국문화예술위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "한국문화정보원: 주목할 만한 기사 없음.\n",
      "\n",
      "한국언론진흥재단: 주목할 만한 기사 없음.\n",
      "\n",
      "한국영상자료원: 주목할 만한 기사 없음.\n",
      "\n",
      "한국콘텐츠진흥원: 주목할 만한 기사 없음.\n",
      "\n",
      "아리랑국제방송: 주목할 만한 기사 없음.\n",
      "\n",
      "영화진흥위원회 영화관입장권 통합전산망: 주목할 만한 기사 없음.\n",
      "\n",
      "한국저작권위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "국립고궁박물관: 주목할 만한 기사 없음.\n",
      "\n",
      "국립무형유산원: 주목할 만한 기사 없음.\n",
      "\n",
      "국립해양문화재연구소: 주목할 만한 기사 없음.\n",
      "\n",
      "국외소재문화재재단: 주목할 만한 기사 없음.\n",
      "\n",
      "한국문화재재단: 주목할 만한 기사 없음.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_of_business_news_contents, tops_of_org_news_contents_splits, result = retrieve_docs(keyword, business_name, crawling_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_of_business_news_contents\n",
    "# tops_of_org_news_contents_splits\n",
    "# result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('jeonghyeon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
