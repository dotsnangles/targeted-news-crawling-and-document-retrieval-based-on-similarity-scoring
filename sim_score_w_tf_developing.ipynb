{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### 모델과 토크나이저를 SentenceTransformer 클래스로 불러옵니다.\n",
    "pretrained_model_path = \"sbert/training_klue_sts_klue-roberta-base-2022-08-17_23-27-13\"\n",
    "model = SentenceTransformer(pretrained_model_path)\n",
    "\n",
    "def id_by_tf_retrieve(df, keyword_th=0, name_th=0):\n",
    "    keyword = df.keyword.unique()[0]\n",
    "    name = df.sub_org.unique()[0]\n",
    "\n",
    "    keyword_tfs = df.content.str.count(keyword).tolist()\n",
    "    name_tfs = df.content.str.count(name).tolist()\n",
    "\n",
    "    tf_pairs = [(q, n) for q, n in zip(keyword_tfs, name_tfs)]\n",
    "    ids = {}\n",
    "    for id, tf_pair in enumerate(tf_pairs):\n",
    "        keyword_tf, name_tf = tf_pair[0], tf_pair[1]\n",
    "        if keyword_tf >= keyword_th and name_tf >= name_th:\n",
    "            ids[id] = keyword_tfs[id] + name_tfs[id]\n",
    "    if bool(ids) == False:\n",
    "        return None, None\n",
    "    \n",
    "    doc_id = max(ids, key=ids.get)\n",
    "    tf_score = max(ids.values())\n",
    "\n",
    "    return doc_id, tf_score\n",
    "\n",
    "### 유사도를 계산하여 인덱스와 스코어를 반환하는 함수입니다.\n",
    "def get_indices_and_scores(query, news_contents, top_k):\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.encode(query)\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "        news_embeddings = model.encode(news_contents.content)\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, news_embeddings).squeeze()\n",
    "\n",
    "        top_k = min(top_k, len(news_contents.content))\n",
    "        top_k_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        scores = top_k_results.values.squeeze()\n",
    "        indices = top_k_results.indices.squeeze()\n",
    "\n",
    "    return indices, scores\n",
    "\n",
    "### 유사도 점수에 기반하여 문서를 찾아옵니다.\n",
    "\n",
    "def retrieve_docs(keyword, business_name, crawling_result):\n",
    "    ### sub_org 칼럼에 business_name 이름이 있는 것과 없는 것을 구분하여 데이터를 나눕니다.\n",
    "    crawling_result['checker'] = crawling_result.sub_org.str.find(business_name)\n",
    "\n",
    "    bussiness_news = crawling_result[crawling_result.checker > -1].copy().reset_index(drop=True)\n",
    "    bussiness_news['idx_original'] = range(len(bussiness_news))\n",
    "\n",
    "    org_news = crawling_result[crawling_result.checker == -1].copy().reset_index(drop=True)\n",
    "    org_news['idx_original'] = range(len(org_news))\n",
    "\n",
    "    business_news_contents = bussiness_news[['idx_original', 'sub_org', 'keyword', 'pubDate', 'title', 'content', 'link']]\n",
    "    org_news_contents = org_news[['idx_original', 'sub_org', 'keyword', 'pubDate', 'title', 'content', 'link']]\n",
    "    \n",
    "    ### 비지니스명과 쿼리의 term frequency threshold를 충족한 기사 중 term frequency의 합계가 가장 높은 기사를 가지고 옵니다.(중복시 첫 번째)\n",
    "    doc_id, tf_score = id_by_tf_retrieve(business_news_contents, keyword_th=1, name_th=2)\n",
    "    if doc_id == None:\n",
    "        print(f'{business_name}: 주목할 만한 기사 없음.')\n",
    "        print('프로그램을 종료합니다.')\n",
    "        return None\n",
    "    else:\n",
    "        top_of_business_news_contents = business_news_contents.iloc[doc_id].to_list() + list([tf_score])\n",
    "        top_of_business_news_contents = pd.DataFrame([top_of_business_news_contents], columns=['idx_original', 'crawling_trg', 'pubDate', 'title', 'content', 'link', 'score'])\n",
    "\n",
    "    ### 기관별로 데이터를 나눕니다.\n",
    "    org_news_contents_splits = []\n",
    "    for org in org_news_contents.sub_org.unique():\n",
    "        org_news_contents_split = org_news_contents[org_news_contents.sub_org == org].reset_index(drop=True).copy()\n",
    "        org_news_contents_splits.append(org_news_contents_split)\n",
    "\n",
    "    ### 각 기관별 수행. 기관명과 쿼리의 term frequency threshold를 충족한 기사 중 term frequency의 합계가 가장 높은 기사를 가지고 옵니다.(중복시 첫 번째)\n",
    "    tops_of_org_news_contents_splits = []\n",
    "    for org_news_contents_split in org_news_contents_splits:\n",
    "        sub_org_name = org_news_contents_split.sub_org.unique()[0]\n",
    "        doc_id, tf_score = id_by_tf_retrieve(org_news_contents_split, keyword_th=2, name_th=3)\n",
    "        if doc_id == None:\n",
    "            print(f'{sub_org_name}: 주목할 만한 기사 없음.')\n",
    "            print()\n",
    "            continue\n",
    "        top_of_org_news_contents_split = org_news_contents_split.iloc[doc_id].to_list() + list([tf_score])\n",
    "        tops_of_org_news_contents_splits.append(top_of_org_news_contents_split)\n",
    "\n",
    "    if len(tops_of_org_news_contents_splits) == 0:\n",
    "        print('검색한 상위 기관에 대하여 주목할 만한 기사 없음.')\n",
    "        print('프로그램을 종료합니다.')\n",
    "        return None\n",
    "    \n",
    "    tops_of_org_news_contents_splits = pd.DataFrame(tops_of_org_news_contents_splits, columns=['idx_original', 'sub_org', 'keyword', 'pubDate', 'title', 'content', 'link', 'score'])\n",
    "    tops_of_org_news_contents_splits = tops_of_org_news_contents_splits.sort_values('score', ascending=False).reset_index(drop=True).copy()\n",
    "\n",
    "    ### 가장 높은 점수의 business_name 문서를 쿼리로 하여 \n",
    "    ### 가장 높은 점수의 기관별 문서 뭉치와 유사도 점수를 계산하고 유사도 점수 상위 5개 문서를 찾아옵니다.\n",
    "    indices, scores = get_indices_and_scores(top_of_business_news_contents.content.iloc[0], tops_of_org_news_contents_splits, 5)\n",
    "    result = tops_of_org_news_contents_splits.iloc[list(indices)].copy()\n",
    "    result['score'] = scores\n",
    "\n",
    "    return top_of_business_news_contents, tops_of_org_news_contents_splits, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['sub_org', 'query', 'pubDate', 'title', 'content', 'originallink', 'link', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = '영화'\n",
    "business_name = '명필름'\n",
    "crawling_result = pd.read_csv('영화_문화체육관광부_명필름_crawled.csv')\n",
    "crawling_result = crawling_result.rename({'query': 'keyword'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'keyword'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_of_business_news_contents, tops_of_org_news_contents_splits, result \u001b[39m=\u001b[39m retrieve_docs(keyword, business_name, crawling_result)\n",
      "Cell \u001b[0;32mIn [113], line 64\u001b[0m, in \u001b[0;36mretrieve_docs\u001b[0;34m(keyword, business_name, crawling_result)\u001b[0m\n\u001b[1;32m     61\u001b[0m org_news_contents \u001b[39m=\u001b[39m org_news[[\u001b[39m'\u001b[39m\u001b[39midx_original\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msub_org\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpubDate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     63\u001b[0m \u001b[39m### 비지니스명과 쿼리의 term frequency threshold를 충족한 기사 중 term frequency의 합계가 가장 높은 기사를 가지고 옵니다.(중복시 첫 번째)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m doc_id, tf_score \u001b[39m=\u001b[39m id_by_tf_retrieve(business_news_contents, keyword_th\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, name_th\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m doc_id \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mbusiness_name\u001b[39m}\u001b[39;00m\u001b[39m: 주목할 만한 기사 없음.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [113], line 12\u001b[0m, in \u001b[0;36mid_by_tf_retrieve\u001b[0;34m(df, keyword_th, name_th)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mid_by_tf_retrieve\u001b[39m(df, keyword_th\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, name_th\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     keyword \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mkeyword\u001b[39m.\u001b[39munique()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m     name \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msub_org\u001b[39m.\u001b[39munique()[\u001b[39m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m     keyword_tfs \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mcount(keyword)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/anaconda3/envs/jeonghyeon/lib/python3.8/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'keyword'"
     ]
    }
   ],
   "source": [
    "top_of_business_news_contents, tops_of_org_news_contents_splits, result = retrieve_docs(keyword, business_name, crawling_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('jeonghyeon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
