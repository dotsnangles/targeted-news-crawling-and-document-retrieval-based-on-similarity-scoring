{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow(query, articles):\n",
    "    name = articles.crawling_trg.unique()[0].split(query)[0].strip()\n",
    "\n",
    "    articles = articles.content.apply(lambda x: x.split()).to_list().copy()\n",
    "    articles = [[word if not (query in word) else query for word in article] for article in articles]\n",
    "    articles = [[word if not (name in word) else name for word in article] for article in articles]\n",
    "\n",
    "    dct = Dictionary(articles)  # fit dictionary\n",
    "    bow_articles = [dct.doc2bow(article) for article in articles]  # convert corpus to BoW format\n",
    "    bow_articles = [{k:v for k, v in bow_article} for bow_article in bow_articles]\n",
    "    \n",
    "    return name, dct, bow_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_by_tf_retrieve(query, name, dct, bow_articles, query_th=0, name_th=0):\n",
    "    query_id = dct.token2id[query]\n",
    "    query_tfs = []\n",
    "    for bow_article in bow_articles:\n",
    "        query_tfs.append(bow_article[query_id])\n",
    "    query_tfs = np.array(query_tfs)\n",
    "\n",
    "    name_id = dct.token2id[name]\n",
    "    name_tfs = []\n",
    "    for bow_article in bow_articles:\n",
    "        name_tfs.append(bow_article[name_id])\n",
    "    name_tfs = np.array(name_tfs)\n",
    "    \n",
    "    tf_pairs = [(q, n) for q, n in zip (query_tfs, name_tfs)]\n",
    "    ids = {}\n",
    "    for id, tf_pair in enumerate(tf_pairs):\n",
    "        query_tf, name_tf = tf_pair[0], tf_pair[1]\n",
    "        if query_tf >= query_th and name_tf >= name_th:\n",
    "            ids[id] = query_tfs[id] + name_tfs[id]\n",
    "    if bool(ids) == False:\n",
    "        return None, None\n",
    "    \n",
    "    doc_id = max(ids, key=ids.get)\n",
    "    tf_score = max(ids.values())\n",
    "\n",
    "    return doc_id, tf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델과 토크나이저를 SentenceTransformer 클래스로 불러옵니다.\n",
    "pretrained_model_path = \"sbert/training_klue_sts_klue-roberta-base-2022-08-17_23-27-13\"\n",
    "model = SentenceTransformer(pretrained_model_path)\n",
    "\n",
    "### 유사도를 계산하여 인덱스와 스코어를 반환하는 함수입니다.\n",
    "def get_indices_and_scores(query, news_contents, top_k):\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.encode(query)\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "        news_embeddings = model.encode(news_contents.content)\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, news_embeddings).squeeze()\n",
    "\n",
    "        top_k = min(top_k, len(news_contents.content))\n",
    "        top_k_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        scores = top_k_results.values.squeeze()\n",
    "        indices = top_k_results.indices.squeeze()\n",
    "\n",
    "    return indices, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 유사도 점수에 기반하여 문서를 찾아옵니다.\n",
    "\n",
    "def retrieve_docs(query, business_name, crawling_result):\n",
    "    ### crawling_trg에 business_name 이름이 있는 것과 없는 것을 구분하여 데이터를 나눕니다.\n",
    "    crawling_result['checker'] = crawling_result.crawling_trg.str.find(business_name)\n",
    "\n",
    "    bussiness_news = crawling_result[crawling_result.checker > -1].copy().reset_index(drop=True)\n",
    "    bussiness_news['idx_original'] = range(len(bussiness_news))\n",
    "\n",
    "    org_news = crawling_result[crawling_result.checker == -1].copy().reset_index(drop=True)\n",
    "    org_news['idx_original'] = range(len(org_news))\n",
    "\n",
    "    business_news_contents = bussiness_news[['idx_original', 'crawling_trg', 'pubDate', 'title', 'content', 'link']]\n",
    "    org_news_contents = org_news[['idx_original', 'crawling_trg', 'pubDate', 'title', 'content', 'link']]\n",
    "    \n",
    "    \n",
    "    ### 비지니스명과 쿼리의 term frequency threshold를 충족한 기사 중 term frequency의 합계가 가장 높은 기사를 가지고 옵니다.(중복시 첫 번째)\n",
    "    name, dct, bow_articles = make_bow(query, business_news_contents)\n",
    "    doc_id, tf_score = id_by_tf_retrieve(query, name, dct, bow_articles, query_th=1, name_th=3)\n",
    "    if doc_id == None:\n",
    "        print(f'{name}: 주목할 만한 기사 없음.')\n",
    "        print('프로그램을 종료합니다.')\n",
    "        return None\n",
    "    else:\n",
    "        top_of_business_news_contents = business_news_contents.iloc[doc_id].to_list() + list([tf_score])\n",
    "        top_of_business_news_contents = pd.DataFrame([top_of_business_news_contents], columns=['idx_original', 'crawling_trg', 'pubDate', 'title', 'content', 'link', 'score'])\n",
    "\n",
    "    ### 기관별로 데이터를 나눕니다.\n",
    "    org_news_contents_splits = []\n",
    "    for org in org_news_contents.crawling_trg.unique():\n",
    "        org_news_contents_split = org_news_contents[org_news_contents.crawling_trg == org].reset_index(drop=True).copy()\n",
    "        org_news_contents_splits.append(org_news_contents_split)\n",
    "\n",
    "    ### 각 기관별 수행. 기관명과 쿼리의 term frequency threshold를 충족한 기사 중 term frequency의 합계가 가장 높은 기사를 가지고 옵니다.(중복시 첫 번째)\n",
    "    tops_of_org_news_contents_splits = []\n",
    "    for org_news_contents_split in org_news_contents_splits:\n",
    "        name, dct, bow_articles = make_bow(query, org_news_contents_split)\n",
    "        doc_id, tf_score = id_by_tf_retrieve(query, name, dct, bow_articles, query_th=2, name_th=3)\n",
    "        if doc_id == None:\n",
    "            print(f'{name}: 주목할 만한 기사 없음.')\n",
    "            print()\n",
    "            continue\n",
    "        top_of_org_news_contents_split = org_news_contents_split.iloc[doc_id].to_list() + list([tf_score])\n",
    "        tops_of_org_news_contents_splits.append(top_of_org_news_contents_split)\n",
    "\n",
    "    if len(tops_of_org_news_contents_splits) == 0:\n",
    "        print('검색한 상위 기관에 대하여 주목할 만한 기사 없음.')\n",
    "        print('프로그램을 종료합니다.')\n",
    "        return None\n",
    "    \n",
    "    tops_of_org_news_contents_splits = pd.DataFrame(tops_of_org_news_contents_splits, columns=['idx_original', 'crawling_trg', 'pubDate', 'title', 'content', 'link', 'score'])\n",
    "    tops_of_org_news_contents_splits = tops_of_org_news_contents_splits.sort_values('score', ascending=False).reset_index(drop=True).copy()\n",
    "\n",
    "    ### 가장 높은 점수의 business_name 문서를 쿼리로 하여 \n",
    "    ### 가장 높은 점수의 기관별 문서 뭉치와 유사도 점수를 계산하고 유사도 점수 상위 5개 문서를 찾아옵니다.\n",
    "    indices, scores = get_indices_and_scores(top_of_business_news_contents.content.iloc[0], tops_of_org_news_contents_splits, 5)\n",
    "    result = tops_of_org_news_contents_splits.iloc[list(indices)].copy()\n",
    "    result['score'] = scores\n",
    "\n",
    "    return top_of_business_news_contents, tops_of_org_news_contents_splits, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawling_result = pd.read_csv('인공지능_문화체육관광부_클라썸_crawled.csv')\n",
    "query = '인공지능'\n",
    "business_name = '클라썸'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서간 유사도 검사를 수행합니다.\n",
      "국립민속박물관: 주목할 만한 기사 없음.\n",
      "\n",
      "국립아시아문화전당: 주목할 만한 기사 없음.\n",
      "\n",
      "국립중앙도서관: 주목할 만한 기사 없음.\n",
      "\n",
      "대한민국역사박물관: 주목할 만한 기사 없음.\n",
      "\n",
      "한국예술종합학교: 주목할 만한 기사 없음.\n",
      "\n",
      "해외문화홍보원: 주목할 만한 기사 없음.\n",
      "\n",
      "게임물관리위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "사행산업통합감독위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "세종학당재단: 주목할 만한 기사 없음.\n",
      "\n",
      "영상물등급위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "영화진흥위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "예술의전당: 주목할 만한 기사 없음.\n",
      "\n",
      "태권도진흥재단: 주목할 만한 기사 없음.\n",
      "\n",
      "한국문화예술교육진흥원: 주목할 만한 기사 없음.\n",
      "\n",
      "한국문화예술위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "한국언론진흥재단: 주목할 만한 기사 없음.\n",
      "\n",
      "한국저작권위원회: 주목할 만한 기사 없음.\n",
      "\n",
      "문화재청: 주목할 만한 기사 없음.\n",
      "\n",
      "국립무형유산원: 주목할 만한 기사 없음.\n",
      "\n",
      "궁능유적본부: 주목할 만한 기사 없음.\n",
      "\n",
      "한국전통문화대학교: 주목할 만한 기사 없음.\n",
      "\n",
      "한국문화재재단: 주목할 만한 기사 없음.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('문서간 유사도 검사를 수행합니다.')\n",
    "top_of_business_news_contents, tops_of_org_news_contents_splits, result = retrieve_docs(query, business_name, crawling_result)\n",
    "\n",
    "# top_of_business_news_contents.to_csv('top_scored_business_news_for_query.csv', index=False, encoding='utf-8-sig')\n",
    "# tops_of_org_news_contents_splits.to_csv('list_of_top_scored_org_news_for_query_by_org.csv', index=False, encoding='utf-8-sig')\n",
    "# result.to_csv('top_5_orgs_and_their_news_for_top_scored_business_news.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# print('문서간 유사도 검사가 완료되었습니다. 다음 파일을 생성했습니다.')\n",
    "# print('top_scored_business_news_for_query.csv')\n",
    "# print('list_of_top_scored_org_news_for_query_by_org.csv')\n",
    "# print('top_5_orgs_and_their_news_for_top_scored_business_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_of_business_news_contents.to_csv('top_scored_business_news_for_query.csv', index=False, encoding='utf-8-sig')\n",
    "tops_of_org_news_contents_splits.to_csv('list_of_top_scored_org_news_for_query_by_org.csv', index=False, encoding='utf-8-sig')\n",
    "result.to_csv('top_5_orgs_and_their_news_for_top_scored_business_news.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_original                                                   76\n",
      "crawling_trg                                         국립중앙박물관 인공지능\n",
      "pubDate                           Fri, 16 Sep 2022 14:50:00 +0900\n",
      "title                                  [문화소식] 16일 수원서 문화재지킴이 전국대회\n",
      "content         국립국어원 '인공지능 언어 능력' 평가 대회 국립중앙박물관 어린이박물관, 데굴데굴 ...\n",
      "link            https://n.news.naver.com/mnews/article/001/001...\n",
      "score                                                     0.53998\n",
      "Name: 2, dtype: object\n",
      "\n",
      "국립국어원 '인공지능 언어 능력' 평가 대회 국립중앙박물관 어린이박물관, 데굴데굴 놀이터' 운영행사 안내 포스터[문화재청 제공. 재판매 및 DB 금지](서울=연합뉴스) 김예나 기자 = ▲ 16일 수원서 문화재지킴이 전국대회 = 문화재청은 한국문화재지킴이단체연합회와 함께 16일 경기 수원 화성 행궁 광장 일대에서 '문화재지킴이 전국대회' 행사를 연다. 문화재지킴이 운동은 국민이 문화재를 자발적으로 가꾸자는 취지로 2005년 시작됐다. 현재 전국에서 6만9천여 명이 문화재 주변 정화, 훼손 방지, 문화재 감시와 홍보 등 다양한 활동을 하고 있다. 전국대회는 이들 지킴이가 한데 모여 활동 공로를 격려하는 자리다. 올해 행사에서는 곳곳의 문화재지킴이 활동을 소개하고 유공자에게 상을 줄 예정이다. 깃발 공연, 국악 공연 등이 열리며 수원 화성을 지켜주는 신을 모신 사당인 '성신사'에서 고유제도 지낼 예정이다. 행사는 문화유산채널 유튜브에서 생중계한다. 행사 안내 포스터[국립국어원 제공. 재판매 및 DB 금지]▲ 국립국어원 '인공지능 언어 능력' 평가 대회 = 국립국어원은 16일부터 '인공지능, 인간의 감성을 이해하다'를 주제로 한 인공지능 언어 능력 평가 대회를 연다. 올해 과제는 속성을 기반으로 한 감성 분석이다. 온라인에서 흔히 볼 수 있는 상품 후기 등과 같은 언어 자료에서 작성자의 주관적 판단이 향하는 대상과 그 속성, 긍정 또는 부정 판단 여부를 인공지능 언어 모델을 이용해 추론한다. 과제 자료는 '모두의 말뭉치'(https:corpus.korean.go.kr)에서 확인할 수 있다. 10월 4일부터 참가 신청을 할 수 있으며, 11월 4일까지 답안을 제출하면 된다. 대상 수상자 1팀에는 문화체육관광부 장관상과 상금 300만 원을 수여하며 금상 1팀, 은상 1팀, 동상 2팀은 국어원장상과 소정의 상금을 받는다. '데굴데굴 놀이터' 전경[국립중앙박물관 제공. 재판매 및 DB 금지]▲ 국립중앙박물관 어린이박물관, 데굴데굴 놀이터' 운영 = 국립중앙박물관 어린이박물관은 16일부터 36개월 이하 영유아를 위한 '데굴데굴 놀이터'를 운영한다. 어린이박물관 안에 위치한 놀이터는 영유아의 발달에 적합한 체험 공간이다. 입구로 들어서면 너비 약 3m의 수레바퀴 모양 토기 구조물이 설치돼 있어 영유아가 오르내릴 수 있다. 마패에 새겨진 말의 수와 숫자를 연결하는 등 수 과학도 배울 수 있다. 팔주령, 청동거울, 황남대총 북분 팔찌의 동그란 모양을 활용해 제작한 체험물도 있다. 놀이터 바깥에 마련된 '아하! 나무'공간에는 그림책이 구비돼 있어 아이들과 함께 책 읽기에 좋다. '데굴데굴 놀이터'는 어린이박물관 전시 관람을 예약하면 입장할 수 있다. yes@yna.co.kr\n",
      "\n",
      "idx_original                                                  184\n",
      "crawling_trg                                          한국관광공사 인공지능\n",
      "pubDate                           Mon, 19 Sep 2022 09:53:00 +0900\n",
      "title                        전국 관광지 정보에 인공지능 알고리즘 도입…‘AI 경진대회’ 개최\n",
      "content         한국관광공사 '관광데이터 AI 경진대회’ 관광지 유형별 자동 분류 AI 알고리즘 아...\n",
      "link            https://n.news.naver.com/mnews/article/277/000...\n",
      "score                                                    0.514386\n",
      "Name: 3, dtype: object\n",
      "\n",
      "한국관광공사 '관광데이터 AI 경진대회’ 관광지 유형별 자동 분류 AI 알고리즘 아이디어 공모한국관광공사는 관광분야 공공데이터를 활용한 인공지능(AI) 알고리즘 개발을 위해 '관광데이터 AI 경진대회'를 개최한다고 19일 밝혔다. 사진제공 = 한국관광공사한국관광공사는 관광분야 공공데이터를 활용한 인공지능(AI) 알고리즘 개발을 위해 '관광데이터 AI 경진대회'를 개최한다고 19일 밝혔다.대회는 다음달 31일까지 국내 최대 데이터분석 플랫폼인 데이콘에서 온라인으로 진행된다. 과제는 공사가 보유한 2만3000여 개의 국문 관광지점정보(POI)의 텍스트와 이미지 데이터를 인공지능 학습 데이터로 활용해 관광 지점의 유형을 자동 분류하는 최적의 알고리즘 개발이다.현재 '대한민국 구석구석' 등 공사의 누리집을 통해 공개되는 관광지 정보는 사람의 손을 거쳐 분류되고 있다. 공사는 인공지능 알고리즘 도입을 통해 이를 자동화함으로써 더 많은 데이터를 생성하고 처리 효율과 검색 능력 개선을 기대하고 있다.심사는 분류 성능을 평가하는 산식과 전문가의 엄정한 코드심사로 진행된다. 대상 1팀(한국관광공사 사장상), 우수상 1팀, 장려상 2팀이 최종 선정된다. 상금은 1000만원 규모로 1등 500만원, 2등 300만원, 3등 각 100만원이 수여된다.AI 알고리즘 개발과 데이터 분석에 관심 있는 사람이라면 개인 또는 5인 이하 팀을 구성해 누구나 참가할 수 있다.조윤미 한국관광공사 관광빅데이터전략팀장은 “현재 공사의 관광지점정보를 적용한 다양한 여행 어플리케이션이 만들어지고 있다”며 “이번 경진대회를 통해 AI를 활용한 우수한 알고리즘이 만들어져, 효율적인 관광정보 관리체계 구축과 개방형 혁신이 이루어지기를 기대한다”고 말했다.\n",
      "\n",
      "idx_original                                                  315\n",
      "crawling_trg                                        한국콘텐츠진흥원 인공지능\n",
      "pubDate                           Sat, 24 Sep 2022 14:01:00 +0900\n",
      "title                 캐릭터 전성시대라지만...호둥이 해랑이는 실패한 까닭 [허태윤 브랜드 스...\n",
      "content         펭수 인기부터 포켓몬빵 열풍까지...캐릭터 시대88올림픽 호돌이부터 2000년대 뽀...\n",
      "link            https://n.news.naver.com/mnews/article/243/000...\n",
      "score                                                    0.495655\n",
      "Name: 6, dtype: object\n",
      "\n",
      "펭수 인기부터 포켓몬빵 열풍까지...캐릭터 시대88올림픽 호돌이부터 2000년대 뽀로로까지인기 캐릭터 많지만, 실패한 캐릭터도 우후죽순 개성과 탄탄한 스토리텔링 없는 캐릭터는 공감↓ 롯데월드타워 광장에 마련된 대형 피카츄 캐릭터. [연합뉴스] 올해 마케팅계를 뒤흔든 사건은 누가 뭐래도 ‘포켓몬 빵’ 열풍을 꼽을 수 있을 것이다. 출시된 지 4개월 만에 무려 1000만개 빵이 팔린 것은 물론이고 물량 부족으로 소비자 성화를 감당하지 못한 일부 편의점주가 불매운동을 벌이는 촌극도 벌어졌다. 심지어 BTS 리더 RM의 부모님이 편의점을 전전하며 빵을 사러 다니는가 하면, 이를 보다 못해 RM이 직접 SNS를 통해 ‘더 많이 팔아달라’고 요청하기도 한 이야기는 유명하다. 그런데 이 포켓몬 빵이 이토록 ‘신드롬’이 된 이유는 세상에 없었던 빵 맛이라서도 아니고, 가성비가 좋아서도 아니다. 빵 포장 안에 들어 있는 포켓몬 캐릭터 스티커를 구하기 위함이었다. 어린 시절 포켓몬스터 애니메이션의 추억과 포켓몬고 AR 게임을 거친 MZ세대들에게 포켓몬 캐릭터는 빵에 부착된 캐릭터로 다시 살아온 것이다. 이른바 ‘캐릭터 전성시대’다. 국민 채팅 앱 카카오에 등장하는 7개 캐릭터들은 이제 국민 캐릭터가 됐다. 한국콘텐츠진흥원의 조사에서 뽀로로, 포켓몬스터를 누르고 한국인이 가장 선호하는 캐릭터로 등극했다. 이들 카카오 프랜즈들이 전 세계에서 올린 IP(지식재산권) 거래액만 2021년 기준, 1조원에 이른다. 이뿐 아니다. 뽀로로에 이어 3위를 한 EBS가 만든 펭귄 ’펭수‘는 어린이 프로의 캐릭터로 출발했지만, 세대를 뛰어넘는 캐릭터가 됐다. 지난 2020년에만 100억원이 넘는 매출을 올려 EBS 방송국의 효자가 됐다. 실제 한국콘텐츠진흥원 자료에 따르면 2005년 2조7000억원이었던 캐릭터 산업의 규모는 2020에 들어 12조원 규모로 15년간 6배 이상 성장했다. EBS 펭귄 캐릭터 펭수가 팬사인회를 진행하고 있다. [연합뉴스] 브랜드들의 캐릭터를 통한 팬덤 형성도 활발하다. 식품회사 빙그레의 ’빙그레우스더마시스‘ 왕자 이야기는 생산하는 주요 제품을 빙그레 왕국이라는 세계관 안에 녹여, 캐릭터 마케팅의 새로운 경지를 열었다. 롯데홈쇼핑의 ’벨리곰‘은 브랜드의 오리지널 콘텐츠 열풍을 불어오게 했고, 70년이 된 곰표 밀가루의 곰표 캐릭터는 패딩, 팝콘, 에 이어 맥주로 만들어지더니 치약, 주방세제까지도곰표의 캐릭터를 붙이자 날개 돛인 듯 팔리고 있다. 이러한 현상은 조사에서도 잘 나타나고 있다. 2021년 한국콘텐츠진흥원 조사에 따르면 3세부터 69세의 응답자 중 62.4%가 상품 구매 때 캐릭터에 영향을 받는다고 답했고, 53%는 캐릭터가 부착된 상품에 추가 비용을 낼 의사가 있다고 답했다. 또한 캐릭터 상품 구매를 경험한 이들은 85.3%에 달하며, 43.5%가 일주일에 1회 이상 캐릭터를 이용한다고 응답했다. ━ 호돌이부터 뽀로로까지, 캐릭터 사회 우리나라 캐릭터 중 대중적 인지도를 가지기 시작한 대표 캐릭터는 88올림픽의 마스코트인 ‘호돌이’다. 88올림픽을 전 세계에 알리고, 국민에게 서울 올림픽에 대한 호감도를 올리기 위해 만들어진 이 캐릭터는 당시의 주역이었던 지금의 장‧노년 세대의 기억에 아직도 생생하게 살아 있을 정도로 성공적이었다. 이후 국민적 캐릭터로 사랑받은 것이 2000년대 초에 등장한 뽀로로다. 부모들이 아무리 달래도 소용없지만, 영유아들의 울음을 한방에 그치게 만들어 ‘뽀통령’이란 별명은 얻은 이 캐릭터는 국산 캐릭터가 글로벌로 진출한 계기가 된 새로운 전환점이다. 영유아 시절을 뽀로로와 함께 보냈던 이들이 자라, 오늘날 캐릭터 소비의 주인공으로 등장한 것이 MZ세대다. 인터넷과 더불어 등장한 SNS의 발달은 캐릭터 산업에 날개를 달아줬다. 사람보다 더 친밀감을 주는 케릭터식 감정 표현에 익숙한 MZ세대가 구매력을 가지게 되자 캐릭터는 산업으로 성장하기 시작한 것이다. TV나 애니메이션이나 영화를 통해서만 인지도를 만들 수 있었던 과거와 달리 SNS 시대에는 어떤 캐릭터도 SNS를 통해 쉽게 퍼지고 공유되는 현상이 일어나면서 누구나 쉽게 캐릭터를 만들고 키워가는 세상이 된 것이다. 그렇다면 브랜드는 왜 캐릭터에 주목할까. 캐릭터는 시각화하기 어려운, 브랜드가 목적하고 지향하는 바를 특징적으로 간결하게 응축한 시각물로 다양한 매체를 통해 소비자에게 전달하기가 쉽기 때문이다. 한 대상을 의인화한 콘텐츠 이기 때문에 다른 콘텐츠 보다 공감이 쉽고 커뮤니케이션이 쉽다는 장점이 있다. 더불어 인간이 아니기에 과장된 개성과 풍부한 이미지를 만들 수 있어 눈에 띄고 기억에 쉽게 남는다는 점과 무엇보다, 하나의 캐릭터로 다양한 매체와 플랫폼의 특성에 따라 다양하게 사용할 수 있는 OSMU(One Source Multi Use)를 통한 확장성이 좋기 때문이다. ━ 캐릭터 성공요인 3가지 하지만 무조건 캐릭터를 만들면 성공할까. 절대 그렇지 않다. 지방자치 단체나 일부 공기업들이 한때 우후죽순처럼 만들어 냈던 수많은 캐릭터 중 상당수가 실패했다. 2018년 한국콘텐츠진흥원 조사에 의하면 전국 공공기관이 보유한 캐릭터는 457개에 이른다. 그런데 해당 지자체는 주민은 물론이고, 이해관계자들도 기억하지도 못하고 사라져간 경우가 대부분이다. 고양시의 고양이와 충주시의 수달 캐릭터 ‘충주씨’ 정도가 살아남았다. 은행권도 캐릭터 개발에는 열심이었지만 비슷한 결과를 보인다. 개성과 특징이 없는 모습으로 실패한 사례로 꼽히는 캐릭터들. 한국중부발전 에코미, 한국소비자원 소망이, 도로교통공단 호둥이, 해양수산부 해랑이. (위부터 시계방향) [사진 화면캡처] 이유는 세 가지로 정리된다. 우선 개성이 없는 경우다. 어디서 봤는지 기억을 못 할 정도로 비슷비슷한 경우가 많다. 한국 중부 발전의 ‘에코미’, 한국 소비자원 ‘소망이’, 도로교통공단 ‘호둥이’, 해양수산부 ‘해랑이’가 대표적이다. 무엇을 상징하는지 설명 없이는 도저히 알 수가 없다. 또 지방의 특산물이나, 지방의 대표 역사 유산 등을 소비자의 선호를 고려치 않고 관료적 리더십으로 일방적으로 만든 경우가 해당한다. 홍길동의 고향이라는 주장을 배경으로 도입된 전남 장성의 캐릭터 홍길동은 20년 동안 4번에 걸친 새로운 홍길동을 탄생시켰는데, 전혀 일관성이 없다. 고객 중심이 아닌 만드는 사람 중심의 캐릭터로 인한 대표적 실패 사례다. 캐릭터 디자인은 성공의 가장 기본적인 요소다. 두 번째로는 설득력 있는 세계관과 지속가능한 스토리텔링이 뒷받침되는 것이 중요하다. 즉 캐릭터에 생명을 불어넣는 작업이다. 이 같은 작업으로 성공한 캐릭터로는 고양시가 2013년 처음 소개한 ‘고양고양이’ 캐릭터가 대표적이다. 우선 동음 이의어인 고양시와 고양이를 직관적으로 연결해 기억하기가 좋았다. 고양시의 '고양고양이' 캐릭터 모습. [사진 화면캡처] 캐릭터 ‘고양고양이’가 태어난 곳은 고양시 덕양구 행주산성으로, 조선 태종 13년 1413년에 출생했으나 적확한 출생일은 밝혀지지 않았다. 캐릭터가 SNS나 블로그 브이로그 등 다양한 채널을 통해 지속해서 대화를 하되 ‘~ 한다고양’, ‘~했고양’ 같은 귀여운 말장난을 통해 귀여운 성격을 불어 넣었다. 젊은 층은 물론이고, 주민들에게 귀여움을 독차지하면서 스토리를 알렸다. 세 번째는 가장 중요한 대중과의 관계 맺기다. 지속해서 대중에게 노출하고 아이디어를 통해 관심을 끌어야 한다. 인지도를 높이는 작업이다. 일본의 구마모토 현을 대표하는 마스코트 '쿠마몬'은 일본이 만든 캐릭터 중 ‘헬로 키티’ 이후 가장 성공한 캐릭터다. 2011년 고속철도 개통을 앞두고 홍보를 위해 탄생한 이 캐릭터는 일본어 쿠마(くま)와 사람을 의미하는 현지 사투리 ‘몬’을 합친 이름이다. 이 캐릭터는 일본 현지에서 헬로키티 이후 가장 성공한 캐릭터라는 평가를 받고 있다. 쿠마몬 등장 이전, 전국의 47개 현 중 하위군인 32위를 차지하던구마모토현의 브랜드 이미지를 10위권으로 끌어올린 것은 물론, 관련 캐릭터 상품이나 굿즈가 1조원 이상 판매된 것으로도 유명하다. 일본 구마모도현의 캐릭터 쿠마몬은 다양한 매체를 통한 스토리와 대중과의 지속적인 소통노력을 통해 인지도를 쌓아 성공한 케이스다. [사진 화면캡처] 비결은 하루도 빠짐없이등장하는 쿠마몬의 소통 노력이다. 구마보도현의영업부장인구마몬은 현의 각종 행사는 물론이고, 방송, 블로그, 유튜브, 현의 각종 상징물에 일관 되게 등장해 대중과의 관계 맺기를 통해 기본적으로 단순하면서도 자유로운 캐릭터의 디자인에 더해 인지도와 호감도를 만든 것이 비결이다. 디지털혁명은 플랫폼, 메타버스, 로봇공학, 인공지능(AI), 4차 산업혁명 등 다양한 과학기술적 진보를 만들어 가고 있다. 이러한 시대에 기술과 과학이 대신 할 수 없는 것은 무엇일까. 점점 더 중요하게 여겨지는 인간이 가진 고유한 감성이다. 캐릭터는 콘텐츠의 ‘인간화’를 염두에 둔 인간을 닮은 콘텐츠다. 캐릭터가 만드는 가상의 세계는 인공지능과 메타버스의 기술에 힘입어 감성과 창조능력을 배가해 더욱 현실 같은 형태로 다가올 것이다. 가상 인플루언서, 메타휴먼 현상이 그 시작이다. 캐릭터 산업이 더욱 인간적인 형태와 모습으로 우리에게 다가올 것이고 대상을 의인화한 캐릭터의 가치가 더욱 빛나게 되는 이유다. ※필자는 칼럼니스트이자 한신대 IT 영상콘텐츠학과 교수다. 광고회사와 공기업, 플랫폼과 스타트업에서 광고와 마케팅을 경험했다. 인도와 미국에서 주재원으로 일하면서 글로벌브랜딩에 관심을 가졌고 공기업 경험으로 공기업 브랜딩, AR과 플랫폼 기업에 관여하면서 플랫폼 브랜딩을 연구하고 있다. 최근에는 2023년 서울에서 열리는 ADASIA 사무총장으로 대회를 준비하고 있다.\n",
      "\n",
      "idx_original                                                  124\n",
      "crawling_trg                                           대한체육회 인공지능\n",
      "pubDate                           Fri, 16 Sep 2022 18:25:00 +0900\n",
      "title                                  속초시, ICT활용 스포츠 컨텐츠 체험시설 유치\n",
      "content         2023년 개장 목표 … 대한체육회와 76억 투입속초시 노학동 척산체육관에 2030...\n",
      "link            https://n.news.naver.com/mnews/article/002/000...\n",
      "score                                                    0.469813\n",
      "Name: 4, dtype: object\n",
      "\n",
      "2023년 개장 목표 … 대한체육회와 76억 투입속초시 노학동 척산체육관에 2030 세대를 겨냥한 스포츠클라이밍, 브레이크댄싱(비보잉), 스케이트보드 등 AI(인공지능)형 체력시설인 'NEXT LEVEL'이 2023년 4월 개장을 목표로 추진된다. ICT(Information Communication ​Technology, 정보통신기술)를 활용한 본 사업은 대한체육회가 지난 4월 4일부터 전국 지자체를 대상으로 공모를 진행하여 공모를 신청한 울산광역시, 대구광역시, 속초시를 대상으로 서류심사, 현지실사 등을 통해 최종 속초시가 선정됐다. ▲속초시 노학동 척산체육관에 2030 세대를 겨냥한 스포츠클라이밍, 브레이크댄싱(비보잉), 스케이트보드 등 AI(인공지능)형 체력시설인 'NEXT LEVEL'이 2023년 4월 개장을 목표로 추진된다. ⓒ속초시 이에 따라 대한체육회와 속초시는 다음 달 중 MOU를 체결하고 연내 본격적인 시설 설치작업에 착수, 노학동 척산생활체육관에 총사업비 76억원(대한체육회 70억 원, 속초시 6억 원)을 투입해 본 시설을 설치･운영할 예정이다. 본 사업이 완료되면 속초시민은 물론 관광객들도 평소에 쉽게 경험하기 힘든 체험형 시설이 제공됨에 따라 속초시가 스포츠와 관광이 연계된 새로운 도시 성장 모델로서 변모하게 될 예정이며, 수도권에 활성화되어 있는 ICT 스포츠 콘텐츠를 지방에서 쉽게 이용할 수 있게 됨에 따라 스포츠 문화 양극화를 해소하고 건강한 여가활동 공간을 제공하여 정주여건 개선에도 일익을 담당하게 될 것으로 기대된다. 이병선 시장은 “대한체육회와 긴밀히 협조해 사업을 계획대로 추진하여 우리 시가 인공지능형 체육시설의 새로운 메카로 거듭날 수 있도록 노력하겠다”고 말했다.\n",
      "\n",
      "idx_original                                                  102\n",
      "crawling_trg                                         국립한글박물관 인공지능\n",
      "pubDate                           Sat, 08 Oct 2022 10:44:00 +0900\n",
      "title                             와이즈넛, `세계 한국어 한마당`서 AI 챗봇 체험 행사\n",
      "content         와이즈넛은 6~9일 국립한글박물관에서 진행되는 '2022 세계 한국어 한마당 - 한...\n",
      "link            https://n.news.naver.com/mnews/article/029/000...\n",
      "score                                                    0.447286\n",
      "Name: 1, dtype: object\n",
      "\n",
      "와이즈넛은 6~9일 국립한글박물관에서 진행되는 '2022 세계 한국어 한마당 - 한글·한국어 산업전'에서 언어처리기술 기반의 인공지능 챗봇을 선보였다. 와이즈넛 제공 인공지능 기업 와이즈넛(대표 강용성)은 6~9일 국립한글박물관에서 진행되는 '2022 세계 한국어 한마당 - 한글·한국어 산업전'에서 언어처리기술 기반의 인공지능 챗봇을 선보였다고 밝혔다. 이 행사는 문화체육관광부와 국립국어원, 국립한글박물관가 주최한다.2022 세계 한국어 한마당은 참가자들의 세대, 거주 지역, 사회 계층 간 소통의 장벽을 허무는 우리말 인공지능 기술에 대해 소개하는 체험의 장을 마련하기 위해 개최됐다.전시회에서 와이즈넛은 자체 개발한 언어처리 기술을 적용한 △구축형(On-premise) 챗봇 '와이즈 아이챗(WISE iChat)' △클라우드형(SaaS) 챗봇 서비스 '현명한 앤써니'를 소개하는 전시부스를 열었다. 특히 일상에서 쉽게 접할 수 있는 사례를 시연해 일반인 참관객의 많은 관심과 참여를 유도했다.대표 사례는 △인천국제공항공사 '에어봇'의 공항 이용 안내 서비스 △강남구청 '강남봇'의 민원상담 챗봇 △신한은행 '오로라'의 고객용 FAQ 챗봇 △CJ대한통운의 택배 문의 원스톱 처리 챗봇 △서울대학교 '스누봇'의 IT상담 챗봇 등이다.와이즈넛의 챗봇은 AI기반 지식관리 측면에서 독자적으로 개발한 '지식 구축 방법론'을 적용했다. 체계적으로 인공지능 챗봇 지식 데이터를 구축하고 최적의 프로세스 구현이 가능하다. 특히 언어 관련 학문을 전공하거나 관련 전담 지식에 대한 사전 이해도가 높은 전문 컨설턴트를 통해 정확도 높은 지식 구축을 수행할 수 있는 장점이 있다.AI 분야의 세계적인 권위를 가진 '국제인공지능학회 AAAI 2021'에서 인공지능 챗봇의 핵심기술을 담은 연구 논문을 발표하기도 했다. 강용성 와이즈넛 대표는 \"전세계적으로 가장 뛰어난 문자로 평가받는 한국어 처리에 가장 큰 경쟁력을 보유한 AI 선도기업으로서 본 행사에 초청을 받아 자체 기술력으로 만든 인공지능 챗봇을 선보이게 됐다\"며 \"앞으로도 와이즈넛은 한글 주도형 인공지능 관련 기술의 지속적인 발전을 위해 노력해 나갈 것\"이라고 말했다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top_of_business_news_contents\n",
    "# tops_of_org_news_contents_splits\n",
    "for idx, row in result.iterrows():\n",
    "    print(row)\n",
    "    print()\n",
    "    print(row.content)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('jeonghyeon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
