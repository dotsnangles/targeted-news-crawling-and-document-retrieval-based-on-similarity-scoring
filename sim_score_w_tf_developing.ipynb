{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow(query, articles):\n",
    "    name = articles.crawling_trg.unique()[0].split(query)[0].strip()\n",
    "\n",
    "    articles = articles.content.apply(lambda x: x.split()).to_list().copy()\n",
    "    articles = [[word if not (query in word) else query for word in article] for article in articles]\n",
    "    articles = [[word if not (name in word) else name for word in article] for article in articles]\n",
    "\n",
    "    dct = Dictionary(articles)  # fit dictionary\n",
    "    bow_articles = [dct.doc2bow(article) for article in articles]  # convert corpus to BoW format\n",
    "    bow_articles = [{k:v for k, v in bow_article} for bow_article in bow_articles]\n",
    "    \n",
    "    return name, dct, bow_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_by_tf_retrieve(query, name, dct, bow_articles, query_th=0, name_th=0):\n",
    "    query_id = dct.token2id[query]\n",
    "    query_tfs = []\n",
    "    for bow_article in bow_articles:\n",
    "        query_tfs.append(bow_article[query_id])\n",
    "    query_tfs = np.array(query_tfs)\n",
    "\n",
    "    name_id = dct.token2id[name]\n",
    "    name_tfs = []\n",
    "    for bow_article in bow_articles:\n",
    "        name_tfs.append(bow_article[name_id])\n",
    "    name_tfs = np.array(name_tfs)\n",
    "    \n",
    "    tf_pairs = [(q, n) for q, n in zip (query_tfs, name_tfs)]\n",
    "    ids = {}\n",
    "    for id, tf_pair in enumerate(tf_pairs):\n",
    "        query_tf, name_tf = tf_pair[0], tf_pair[1]\n",
    "        if query_tf >= query_th and name_tf >= name_th:\n",
    "            ids[id] = query_tfs[id] + name_tfs[id]\n",
    "    if bool(ids) == False:\n",
    "        return None, None\n",
    "    \n",
    "    doc_id = max(ids, key=ids.get)\n",
    "    tf_score = max(ids.values())\n",
    "\n",
    "    return doc_id, tf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    name, dct, bow_articles = make_bow(query, business_news_contents)\n",
    "    doc_id, tf_score = id_by_tf_retrieve(query, name, dct, bow_articles, query_th=1, name_th=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델과 토크나이저를 SentenceTransformer 클래스로 불러옵니다.\n",
    "pretrained_model_path = \"sbert/training_klue_sts_klue-roberta-base-2022-08-17_23-27-13\"\n",
    "model = SentenceTransformer(pretrained_model_path)\n",
    "\n",
    "### 유사도를 계산하여 인덱스와 스코어를 반환하는 함수입니다.\n",
    "def get_indices_and_scores(query, news_contents, top_k):\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.encode(query)\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "        news_embeddings = model.encode(news_contents.content)\n",
    "        cos_scores = util.pytorch_cos_sim(query_embedding, news_embeddings).squeeze()\n",
    "\n",
    "        top_k = min(top_k, len(news_contents.content))\n",
    "        top_k_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        scores = top_k_results.values.squeeze()\n",
    "        indices = top_k_results.indices.squeeze()\n",
    "\n",
    "    return indices, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 유사도 점수에 기반하여 문서를 찾아옵니다.\n",
    "\n",
    "def retrieve_docs(query, business_name, crawling_result):\n",
    "    ### crawling_trg에 business_name 이름이 있는 것과 없는 것을 구분하여 데이터를 나눕니다.\n",
    "    crawling_result['checker'] = crawling_result.crawling_trg.str.find(business_name)\n",
    "\n",
    "    bussiness_news = crawling_result[crawling_result.checker > -1].copy().reset_index(drop=True)\n",
    "    bussiness_news['idx_original'] = range(len(bussiness_news))\n",
    "\n",
    "    org_news = crawling_result[crawling_result.checker == -1].copy().reset_index(drop=True)\n",
    "    org_news['idx_original'] = range(len(org_news))\n",
    "\n",
    "    business_news_contents = bussiness_news[['idx_original', 'crawling_trg', 'pubDate', 'title', 'content', 'link']]\n",
    "    org_news_contents = org_news[['idx_original', 'crawling_trg', 'pubDate', 'title', 'content', 'link']]\n",
    "    \n",
    "    \n",
    "    ### 비지니스명과 쿼리의 term frequency threshold를 충족한 기사 중 term frequency의 합계가 가장 높은 기사를 가지고 옵니다.(중복시 첫 번째)\n",
    "    name, dct, bow_articles = make_bow(query, business_news_contents)\n",
    "    doc_id, tf_score = id_by_tf_retrieve(query, name, dct, bow_articles, query_th=1, name_th=3)\n",
    "    if doc_id == None:\n",
    "        print(f'{name}: 주목할 만한 기사 없음.')\n",
    "        print('프로그램을 종료합니다.')\n",
    "        return None\n",
    "    else:\n",
    "        top_of_business_news_contents = business_news_contents.iloc[doc_id].to_list() + list([tf_score])\n",
    "        top_of_business_news_contents = pd.DataFrame([top_of_business_news_contents], columns=['idx_original', 'crawling_trg', 'pubDate', 'title', 'content', 'link', 'score'])\n",
    "\n",
    "    ### 기관별로 데이터를 나눕니다.\n",
    "    org_news_contents_splits = []\n",
    "    for org in org_news_contents.crawling_trg.unique():\n",
    "        org_news_contents_split = org_news_contents[org_news_contents.crawling_trg == org].reset_index(drop=True).copy()\n",
    "        org_news_contents_splits.append(org_news_contents_split)\n",
    "\n",
    "    ### 각 기관별 수행. 기관명과 쿼리의 term frequency threshold를 충족한 기사 중 term frequency의 합계가 가장 높은 기사를 가지고 옵니다.(중복시 첫 번째)\n",
    "    tops_of_org_news_contents_splits = []\n",
    "    for org_news_contents_split in org_news_contents_splits:\n",
    "        name, dct, bow_articles = make_bow(query, org_news_contents_split)\n",
    "        doc_id, tf_score = id_by_tf_retrieve(query, name, dct, bow_articles, query_th=2, name_th=3)\n",
    "        if doc_id == None:\n",
    "            print(f'{name}: 주목할 만한 기사 없음.')\n",
    "            print()\n",
    "            continue\n",
    "        top_of_org_news_contents_split = org_news_contents_split.iloc[doc_id].to_list() + list([tf_score])\n",
    "        tops_of_org_news_contents_splits.append(top_of_org_news_contents_split)\n",
    "\n",
    "    if len(tops_of_org_news_contents_splits) == 0:\n",
    "        print('검색한 상위 기관에 대하여 주목할 만한 기사 없음.')\n",
    "        print('프로그램을 종료합니다.')\n",
    "        return None\n",
    "    \n",
    "    tops_of_org_news_contents_splits = pd.DataFrame(tops_of_org_news_contents_splits, columns=['idx_original', 'crawling_trg', 'pubDate', 'title', 'content', 'link', 'score'])\n",
    "    tops_of_org_news_contents_splits = tops_of_org_news_contents_splits.sort_values('score', ascending=False).reset_index(drop=True).copy()\n",
    "\n",
    "    ### 가장 높은 점수의 business_name 문서를 쿼리로 하여 \n",
    "    ### 가장 높은 점수의 기관별 문서 뭉치와 유사도 점수를 계산하고 유사도 점수 상위 5개 문서를 찾아옵니다.\n",
    "    indices, scores = get_indices_and_scores(top_of_business_news_contents.content.iloc[0], tops_of_org_news_contents_splits, 5)\n",
    "    result = tops_of_org_news_contents_splits.iloc[list(indices)].copy()\n",
    "    result['score'] = scores\n",
    "\n",
    "    return top_of_business_news_contents, tops_of_org_news_contents_splits, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawling_result = pd.read_csv('인공지능_문화체육관광부_클라썸_crawled.csv')\n",
    "query = '인공지능'\n",
    "business_name = '클라썸'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('문서간 유사도 검사를 수행합니다.')\n",
    "top_of_business_news_contents, tops_of_org_news_contents_splits, result = retrieve_docs(query, business_name, crawling_result)\n",
    "\n",
    "# top_of_business_news_contents.to_csv('top_scored_business_news_for_query.csv', index=False, encoding='utf-8-sig')\n",
    "# tops_of_org_news_contents_splits.to_csv('list_of_top_scored_org_news_for_query_by_org.csv', index=False, encoding='utf-8-sig')\n",
    "# result.to_csv('top_5_orgs_and_their_news_for_top_scored_business_news.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# print('문서간 유사도 검사가 완료되었습니다. 다음 파일을 생성했습니다.')\n",
    "# print('top_scored_business_news_for_query.csv')\n",
    "# print('list_of_top_scored_org_news_for_query_by_org.csv')\n",
    "# print('top_5_orgs_and_their_news_for_top_scored_business_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_of_business_news_contents.to_csv('top_scored_business_news_for_query.csv', index=False, encoding='utf-8-sig')\n",
    "tops_of_org_news_contents_splits.to_csv('list_of_top_scored_org_news_for_query_by_org.csv', index=False, encoding='utf-8-sig')\n",
    "result.to_csv('top_5_orgs_and_their_news_for_top_scored_business_news.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_of_business_news_contents\n",
    "# tops_of_org_news_contents_splits\n",
    "for idx, row in result.iterrows():\n",
    "    print(row)\n",
    "    print()\n",
    "    print(row.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traceback (most recent call last):\n",
    "#   File \"main.py\", line 87, in <module>\n",
    "#     main()\n",
    "#   File \"main.py\", line 75, in main\n",
    "#     top_of_business_news_contents, tops_of_org_news_contents_splits, result = retrieve_docs(query, business_name, crawling_result)\n",
    "#   File \"/home/ubuntu/jongmin/Jeonghyeon/codes/news_crawler/module/retrieve.py\", line 104, in retrieve_docs\n",
    "#     doc_id, tf_score = id_by_tf_retrieve(query, name, dct, bow_articles, query_th=2, name_th=3)\n",
    "#   File \"/home/ubuntu/jongmin/Jeonghyeon/codes/news_crawler/module/retrieve.py\", line 34, in id_by_tf_retrieve\n",
    "#     name_tfs.append(bow_article[name_id])\n",
    "# KeyError: 39\n",
    "# (jeonghyeon) ubuntu@gpu-1:~/jongmin/Jeonghyeon/codes/news_crawler$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from module.crawler import get_news_list, crawl_news\n",
    "from module.preprocess import preprocess\n",
    "from module.utils import filter_time\n",
    "from module.retrieve import retrieve_docs, make_bow, id_by_tf_retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '영화'\n",
    "business_name = '명필름'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국립국어원\n",
      "국립민속박물관\n",
      "국립아시아문화전당\n",
      "국립장애인도서관\n",
      "국립중앙도서관\n",
      "국립중앙박물관\n",
      "국립한글박물관\n",
      "대한민국역사박물관\n",
      "한국예술종합학교\n",
      "해외문화홍보원\n",
      "게임물관리위원회\n",
      "국립박물관문화재단\n",
      "대한체육회\n",
      "사행산업통합감독위원회\n",
      "세종학당재단\n",
      "영상물등급위원회\n",
      "영화진흥위원회\n",
      "예술의전당\n",
      "재단법인국악방송\n",
      "태권도진흥재단\n",
      "한국관광공사\n",
      "한국문학번역원\n",
      "한국문화예술교육진흥원\n",
      "한국문화예술위원회\n",
      "한국문화정보원\n",
      "한국언론진흥재단\n",
      "한국영상자료원\n",
      "한국콘텐츠진흥원\n",
      "아리랑국제방송\n",
      "영화진흥위원회 기획개발지원센터\n",
      "영화진흥위원회 영화관입장권 통합전산망\n",
      "영화진흥위원회 한국영화아카데미\n",
      "영화진흥위원회 한국영화해외진출플랫폼\n",
      "한국저작권위원회\n",
      "문화재청\n",
      "국립고궁박물관\n",
      "국립무형유산원\n",
      "국립해양문화재연구소\n",
      "궁능유적본부\n",
      "한국전통문화대학교\n",
      "국외소재문화재재단\n",
      "한국문화재재단\n",
      "명필름\n"
     ]
    }
   ],
   "source": [
    "def temp(x):\n",
    "    x = x.split()\n",
    "    name = ' '.join(x[:-1])\n",
    "    query = '[SEP]' + x[-1]\n",
    "    return name+query\n",
    "\n",
    "crawling_result = pd.read_csv('영화_문화체육관광부_명필름_crawled.csv')\n",
    "crawling_result['crawling_trg'] = crawling_result.crawling_trg.apply(temp)\n",
    "\n",
    "dfs = []\n",
    "for trg in list(crawling_result.crawling_trg.unique()):\n",
    "    dfs.append(crawling_result[crawling_result.crawling_trg == trg])\n",
    "    \n",
    "new_dfs = []\n",
    "for df in dfs:\n",
    "    sub_org = df.crawling_trg.unique()[0].split('[SEP]')[0]\n",
    "    print(sub_org)\n",
    "    search_sub_org = df.content.str.contains(sub_org, case=False, regex=True)\n",
    "    search_query = df.content.str.contains(query, case=False, regex=True)\n",
    "    df = df[search_sub_org & search_query].copy().reset_index(drop=True)\n",
    "    if len(df) > 0:\n",
    "        new_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'무슨 일인가 일인가'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp('무슨 일인가 일인가 일').split('[SEP]')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawling_result = pd.concat(new_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = crawling_result[crawling_result.crawling_trg == '영화진흥위원회[SEP]영화']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_tf = test.content.str.count('영화진흥위원회').tolist()\n",
    "query_tf = test.content.str.count('영화').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_th = 1\n",
    "name_th = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 1, 1, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 3, 2, 1, 1, 1, 1, 2, 2, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1, 2, 1, 2, 9, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 5, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(name_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국립국어원\n",
      "국립국어원\n",
      "국립민속박물관\n",
      "국립민속박물관\n",
      "국립아시아문화전당\n",
      "국립아시아문화전당\n",
      "39\n",
      "국립장애인도서관\n",
      "국립장애인도서관\n",
      "국립중앙도서관\n",
      "국립중앙도서관\n",
      "국립중앙박물관\n",
      "국립중앙박물관\n",
      "국립한글박물관\n",
      "국립한글박물관\n",
      "대한민국역사박물관\n",
      "대한민국역사박물관\n",
      "한국예술종합학교\n",
      "한국예술종합학교\n",
      "해외문화홍보원\n",
      "해외문화홍보원\n",
      "게임물관리위원회\n",
      "게임물관리위원회\n",
      "국립박물관문화재단\n",
      "국립박물관문화재단\n",
      "대한체육회\n",
      "대한체육회\n",
      "사행산업통합감독위원회\n",
      "사행산업통합감독위원회\n",
      "세종학당재단\n",
      "세종학당재단\n",
      "영상물등급위원회\n",
      "영상물등급위원회\n",
      "영화진흥위원회\n",
      "영화진흥위원회\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'영화진흥위원회'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m new_dfs:\n\u001b[1;32m      2\u001b[0m     name, dct, bow_articles \u001b[39m=\u001b[39m make_bow(query, df)\n\u001b[0;32m----> 3\u001b[0m     doc_id, tf_score \u001b[39m=\u001b[39m id_by_tf_retrieve(query, name, dct, bow_articles, query_th\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, name_th\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m~/jongmin/Jeonghyeon/codes/news_crawler/module/retrieve.py:34\u001b[0m, in \u001b[0;36mid_by_tf_retrieve\u001b[0;34m(query, name, dct, bow_articles, query_th, name_th)\u001b[0m\n\u001b[1;32m     31\u001b[0m     query_tfs\u001b[39m.\u001b[39mappend(bow_article[query_id])\n\u001b[1;32m     32\u001b[0m query_tfs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(query_tfs)\n\u001b[0;32m---> 34\u001b[0m name_id \u001b[39m=\u001b[39m dct\u001b[39m.\u001b[39;49mtoken2id[name]\n\u001b[1;32m     35\u001b[0m name_tfs \u001b[39m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m bow_article \u001b[39min\u001b[39;00m bow_articles:\n",
      "\u001b[0;31mKeyError\u001b[0m: '영화진흥위원회'"
     ]
    }
   ],
   "source": [
    "for df in new_dfs:\n",
    "    name, dct, bow_articles = make_bow(query, df)\n",
    "    doc_id, tf_score = id_by_tf_retrieve(query, name, dct, bow_articles, query_th=2, name_th=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_of_business_news_contents, tops_of_org_news_contents_splits, result = retrieve_docs(query, business_name, crawling_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name ='무엇 인가'\n",
    "''.join(name.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('영화_문화체육관광부_명필름_crawled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n",
    "data = data[['crawling_trg', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.crawling_trg == f'{business_name} 영화']\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sub_org = data.content.str.contains(business_name, case=False, regex=True)\n",
    "search_query = data.content.str.contains(query, case=False, regex=True)\n",
    "data = data[search_sub_org & search_query].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('jeonghyeon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
